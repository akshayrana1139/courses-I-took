# reinforcement-couse-david-silver

<b>Exploration & Exploitation</b>

The best long-term strategy may involve short-term strategy. 

State action exploration
- Systematically explore state space/action space, e.g pick diffrnt action A each time S is visited. 

Parameter exploration 
- Paramterize policy &pi;(A|S, &mu;)
- Try dffrnt parameters and try for a while. 

<h2><b>Multi Armed Bandits</b> : </h2>  

Tuple <A,R> At each time step t, the agent selects an action A and the environment generates a reward R, and aim is to maximize the cumulative reward. 

The action value is q(a) = E{R|A = a}  
The optimal value is v* = q(a<sup>*</sup>) = max q(a)  
The regret is how bad we did l<sub>t</sub> = v* - q(a)  
Total regret is cumulative L<sub>t</sub> = &Sigma;<sub>t = 1->t</sub> (v* - q(a<sub>t</sub>))

<h3><b>Without Bayesian</b> : </h3>  


Gap : &Delta;a = v* - q(a)  , N = no. of selections for action a   
Regret changes to function of gap and count.  
L<sub>t</sub> = &Sigma; <sub>a = A</sub> E [ N(a) ] &Delta;a

Good algorithm requires small count for large gaps. If an algorithm explores forever/never, it will have a linear total regret loss over time steps. So is it possible to achieve sublinear total loss??

- Greedy algorithm selects the max of mean of all actions (Monte Carlo evaluation) and can thus lock onto the suboptimal action forever and thus has a linear total regret. 

- Optimistic Greedy intitializes all value to maximum ( meaning all actions are good ), and then act greedily and if the action is terrible, you need to play it a lot many times to bring down the mean so this kinda encourages exporation unless really proven to be bad/sub-optimal, bt again few unlucky samples can lock out optimal action forever and thus has a linear total regret. 

- &epsilon;-greedy : even here we look for random action in a coin toss - so which means we are increasing the regret linearly with time steps. 

- softmax exploration : similarly for this too it is linear 

- <b>Decay &epsilon;-greedy</b> : If u just decay ur epsilon over time, then we can get asymptotic total regret! - We just wanna explore more when the gaps are less and explore less when gaps are large., but the way we decay, we require advance knowledge of gaps but our goal was to achieve sublinear regret without knowledge of R.

- <b>Upper Confidence Bound UCB </b>  
Estimate an upper confidence U<sub>t</sub>(a) for each action value, such that  
q(a) <= Q<sub>t</sub>(a) + U<sub>t</sub>(a)  
so we select action maximising the UCB i.e. max Q<sub>t</sub>(a) + U<sub>t</sub>(a)   
small N(a) -> large U<sub>t</sub>(a) : coz uncertain since tried less  
large N(a) -> less U<sub>t</sub>(a) : coz certain since tried alot  
Eventually we end up just using the mean Q<sub>t</sub>(a) coz now we are certain about the action and UCB goes down     
Applying Hoeffding's Inequality.. Leading to UCB1 algorithm   
A<sub>t</sub> = argmax Q<sub>t</sub>(a) + (2 log t / N(a)))<sup>1/2</sup>



<h3><b>Bayesian</b> : </h3>  

These exploit prior knowledge about rewards using diffrnt algorithms. 
- Probability matching: selects action a acc to probablity that a is an optimal action. 
    - <b>Thompson Sampling</b>: Asymptotic optimal and achieves the lower bound on total regret.

- Information State Space : Exploration is useful to gain information and we think of quanitfying the information! Therefore it makes sense to explore uncertain situations more if we know value of information, we can trade-off exploration and exploitation optimally.    
At each step there is an information state s~ , and each action a causes a transition to a new information state s~ by adding information)  
The information state is s~ = <&alpha; , &beta;>  
&alpha; counts the pulls of arm a where reward was 0  
&beta; counts the pulls of arm a where reward was 1

    Bayes Adaptive RL : can be solved by Dynammic Programming and known as Gittins index. 

<h2><b>Contextual Bandits</b> : </h2>  

Tuple <A, S, R>


<h2><b>MDPs</b> : </h2>  

All the previous ideas can be extended here and the UCB approach can be generalize here as well. 


Information State space in MDP:   
MDPs can be augmented to include information state Now the augmented state is < S, S~>.   
Each action A causes a transition to state S~ and state S








